<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Overview</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>



<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>Overview</h1>

<p>The purpose of this R markdown file is to document the the metadata analysis, assumptions and process of Getting and Cleaning the dataset into a single data file as the submission requirements for this project.</p>

<p>The R codes within this Markdown will subsequently be extracted and placed into the run_analysis.R for submission purpose.  </p>

<h1>Loading the data for metadata analysis</h1>

<p>As there are no proper column labels or documentation on the data contained within each file and its stucture, there is a need to generate some metadata on the dataset to understand how these files are related to each other, so as to define the strategy to merge them.  Also, there is a need to document some of the assumptions made about the files&#39; structure, and thus the approach in assigning the labels to each column for the final output file.</p>

<pre><code class="r">## Load the libraries
library(data.table)
library(dplyr)
library(RCurl)

## Download the file from the URL provided in the project brief
if (!file.exists(&quot;./data&quot;)) dir.create(&quot;./data&quot;)

if (!file.exists(&quot;./data/UCI HAR Dataset&quot;)) {
    fileURL &lt;- 
        &quot;https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip&quot;
    targetURL &lt;- &quot;./data/UCI HAR Dataset.zip&quot;
    setInternet2(TRUE)
    download.file(fileURL, destfile = targetURL)
    unzip(targetURL, exdir=&quot;./data&quot;)
}

## Read all the data files into memory

# Read reference files
activity_labels &lt;- read.table(&quot;./data/UCI HAR Dataset/activity_labels.txt&quot;, 
                              header=FALSE, quote=&quot;\&quot;&quot;, 
                              stringsAsFactors=FALSE)
features &lt;- read.table(&quot;./data/UCI HAR Dataset/features.txt&quot;,
                       header=FALSE, quote=&quot;\&quot;&quot;)

# Read training data files
y_train &lt;- read.table(&quot;./data/UCI HAR Dataset/train/y_train.txt&quot;, 
                      header=FALSE, quote=&quot;\&quot;&quot;)
x_train &lt;- read.table(&quot;./data/UCI HAR Dataset/train/X_train.txt&quot;, 
                      header=FALSE, quote=&quot;\&quot;&quot;)
subject_train &lt;- read.table(&quot;./data/UCI HAR Dataset/train/subject_train.txt&quot;, 
                            header=FALSE, quote=&quot;\&quot;&quot;)

# Read training Inertial data files
total_acc_z_train &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/train/Inertial Signals/total_acc_z_train.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
total_acc_y_train &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/train/Inertial Signals/total_acc_y_train.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
total_acc_x_train &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/train/Inertial Signals/total_acc_x_train.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
body_gyro_z_train &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/train/Inertial Signals/body_gyro_z_train.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
body_gyro_y_train &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/train/Inertial Signals/body_gyro_y_train.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
body_gyro_x_train &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/train/Inertial Signals/body_gyro_x_train.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
body_acc_z_train &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/train/Inertial Signals/body_acc_z_train.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
body_acc_y_train &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/train/Inertial Signals/body_acc_y_train.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
body_acc_x_train &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/train/Inertial Signals/body_acc_x_train.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)

# Read testing data files
y_test &lt;- read.table(&quot;./data/UCI HAR Dataset/test/y_test.txt&quot;, 
                      header=FALSE, quote=&quot;\&quot;&quot;)
x_test &lt;- read.table(&quot;./data/UCI HAR Dataset/test/X_test.txt&quot;, 
                      header=FALSE, quote=&quot;\&quot;&quot;)
subject_test &lt;- read.table(&quot;./data/UCI HAR Dataset/test/subject_test.txt&quot;, 
                            header=FALSE, quote=&quot;\&quot;&quot;)

# Read testing Inertial data files
total_acc_z_test &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/test/Inertial Signals/total_acc_z_test.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
total_acc_y_test &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/test/Inertial Signals/total_acc_y_test.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
total_acc_x_test &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/test/Inertial Signals/total_acc_x_test.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
body_gyro_z_test &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/test/Inertial Signals/body_gyro_z_test.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
body_gyro_y_test &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/test/Inertial Signals/body_gyro_y_test.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
body_gyro_x_test &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/test/Inertial Signals/body_gyro_x_test.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
body_acc_z_test &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/test/Inertial Signals/body_acc_z_test.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
body_acc_y_test &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/test/Inertial Signals/body_acc_y_test.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
body_acc_x_test &lt;- 
    read.table(&quot;./data/UCI HAR Dataset/test/Inertial Signals/body_acc_x_test.txt&quot;,
               header=FALSE, quote=&quot;\&quot;&quot;)
</code></pre>

<h2>Generate the metadata for analysis and comparing with information we already have.</h2>

<h3>Information provided in the project</h3>

<ol>
<li> Number of Subjects: 30</li>
<li> Data apportionment between Training and Test:  70% &amp; 30%</li>
<li> Number of Feature Domain Variables (with time &amp; frequency): 561</li>
<li> Number of Activities measured: 9</li>
<li> Total number of samples (readings/window) for GIRO sensor in body_giro_xyz (3 separate files): 128 per record</li>
<li> Total number of samples (readings/window) for Acceleration sensor body_acc_xyz (3 separate files): 128 per record</li>
<li> Total number of samples (readings/window) for Triaxial acceleration from the accelerometer total_acc_xyz (3 separate files): 128 per record</li>
</ol>

<h3>Information dervied from above</h3>

<ol>
<li> Number of Subjects in Training (70% of 30 Subjects):    21</li>
<li> Number of Subjects in test (30% of 30 Subjects): 9</li>
</ol>

<h3>Metadata that can be derived from the datasets provided</h3>

<p>The following outlines the key observations made on the dataset provided. It does not 
contain all observations made, just those that provides the background rationale
on the assumptions that needs to be made to support the consolidation of the various
dataset files into a single data file  </p>

<h4>Generating metadata from the first two folder levels of datasets provided</h4>

<pre><code class="r">## Metadata of activity_labels
str(activity_labels)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    6 obs. of  2 variables:
##  $ V1: int  1 2 3 4 5 6
##  $ V2: chr  &quot;WALKING&quot; &quot;WALKING_UPSTAIRS&quot; &quot;WALKING_DOWNSTAIRS&quot; &quot;SITTING&quot; ...
</code></pre>

<pre><code class="r">unique(activity_labels)
</code></pre>

<pre><code>##   V1                 V2
## 1  1            WALKING
## 2  2   WALKING_UPSTAIRS
## 3  3 WALKING_DOWNSTAIRS
## 4  4            SITTING
## 5  5           STANDING
## 6  6             LAYING
</code></pre>

<pre><code class="r">## Metadata of features
str(features)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    561 obs. of  2 variables:
##  $ V1: int  1 2 3 4 5 6 7 8 9 10 ...
##  $ V2: Factor w/ 477 levels &quot;angle(tBodyAccJerkMean),gravityMean)&quot;,..: 243 244 245 250 251 252 237 238 239 240 ...
</code></pre>

<pre><code class="r">nrow(unique(features))
</code></pre>

<pre><code>## [1] 561
</code></pre>

<pre><code class="r">## Metadata of x_train and x_test
dim(x_train)
</code></pre>

<pre><code>## [1] 7352  561
</code></pre>

<pre><code class="r">dim(x_test)
</code></pre>

<pre><code>## [1] 2947  561
</code></pre>

<pre><code class="r">## Percentage of Train and Test observations
nrow(x_train)/(nrow(x_train) + nrow(x_test)) * 100
</code></pre>

<pre><code>## [1] 71.38557
</code></pre>

<pre><code class="r">nrow(x_test)/(nrow(x_train) + nrow(x_test)) * 100
</code></pre>

<pre><code>## [1] 28.61443
</code></pre>

<pre><code class="r">## Metadata of y_train and y_test
str(y_train)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    7352 obs. of  1 variable:
##  $ V1: int  5 5 5 5 5 5 5 5 5 5 ...
</code></pre>

<pre><code class="r">str(y_test)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    2947 obs. of  1 variable:
##  $ V1: int  5 5 5 5 5 5 5 5 5 5 ...
</code></pre>

<pre><code class="r">## Unique values from y_train and y_test
unique(y_train)         # If values matches those in activities_labels, this could be the Activities
</code></pre>

<pre><code>##     V1
## 1    5
## 28   4
## 52   6
## 79   1
## 126  3
## 151  2
</code></pre>

<pre><code class="r">unique(y_test)
</code></pre>

<pre><code>##     V1
## 1    5
## 32   4
## 56   6
## 80   1
## 110  3
## 134  2
</code></pre>

<pre><code class="r">## Unique values from y_train and y_test
nrow(unique(y_train))   # If it is 6 unique observation, this could be the Activities
</code></pre>

<pre><code>## [1] 6
</code></pre>

<pre><code class="r">nrow(unique(y_test))    
</code></pre>

<pre><code>## [1] 6
</code></pre>

<pre><code class="r">## Metadata of subject_train and subject_test
str(subject_train)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    7352 obs. of  1 variable:
##  $ V1: int  1 1 1 1 1 1 1 1 1 1 ...
</code></pre>

<pre><code class="r">str(subject_test)
</code></pre>

<pre><code>## &#39;data.frame&#39;:    2947 obs. of  1 variable:
##  $ V1: int  2 2 2 2 2 2 2 2 2 2 ...
</code></pre>

<pre><code class="r">## Unique values from subject_train and subject_test
unique(subject_train$V1)    # Are values between 1 to 30? If so, these are the subjects
</code></pre>

<pre><code>##  [1]  1  3  5  6  7  8 11 14 15 16 17 19 21 22 23 25 26 27 28 29 30
</code></pre>

<pre><code class="r">unique(subject_test$V1)
</code></pre>

<pre><code>## [1]  2  4  9 10 12 13 18 20 24
</code></pre>

<pre><code class="r">## Number of unique values from subject_train and subject_test
length(unique(subject_train$V1))
</code></pre>

<pre><code>## [1] 21
</code></pre>

<pre><code class="r">length(unique(subject_test$V1))
</code></pre>

<pre><code>## [1] 9
</code></pre>

<h3>Analysis of Metadata of the first two folder levels of datasets</h3>

<ol>
<li> &ldquo;activity_labels&rdquo; contains all the activities being measured</li>
<li> There is a unique numeric value (integer) assigned to each activity label - this potentially would be an index value for the activity.  And can be used to link to the data.</li>
<li> &ldquo;features&rdquo; contains all the features being measured or calculated in total 561</li>
<li> There is a unique numeric value (integer) assigned to each feature - this potentially would be an index value for the feature.  And can be used to link to the data.</li>
<li> There are 7352 observations and 561 variables in X_train file.  The number of variables matches the number of features being measured or calculated.  However, there is no column headers to denote which column is for which feature.</li>
<li> There are 2947 observations and 561 variables in X_test file. The number of variables matches the number of features being measured or calculated.   However, there is no column headers to denote which column is for which feature.</li>
<li> There are <strong>no</strong> potential index values found in both X_train and X_test files, based on sample data shown.</li>
<li> The number of observations in X_train and X_test files are approximately 70%/30% (respectively) of the total observations made in both files</li>
<li> There are 7532 obseravtions of 1 variable in y_train, and 2947 observations of 1 variable in y_test file.</li>
<li>There are 6 unique values ranging from 1 to 6 in both y_train and y_test files.  Potentially, each of this would reference the index found in the activity_labels file.</li>
<li>Also, as there are matching number of observations between y_train with x_train, and y_test with x_test, potentially, each observation in the &ldquo;y&rdquo; files would match one observation in the &ldquo;X&rdquo; files.  However, there are no key in the &ldquo;X&rdquo; files to determine which observation in it would match which observation in the &ldquo;y&rdquo; files.  An assumption would need to be made here to link these two set of files.</li>
<li> There are 7352 observations of 1 variable made in subject_train file, and 2947 observations of 1 variable made in the subject_test files.</li>
<li>There are 21 unique values ranging from 1 to 30 in subject_train file, and 9 unique values ranging from 2 to 24.  Given that there are 30 Subjects in this experiment, potentially, each of these unique value (subject_train and subject_test) refers to a Subject.</li>
<li>Also, as there are matching number of observations between the &ldquo;subject&rdquo; files with the &ldquo;X&rdquo; set of files, potentially each of this observation matches an observation in the &ldquo;X&rdquo; set of files.  However, there are no key in the &ldquo;X&rdquo; files to determine which observation in it would match the Subject identifier in the &ldquo;subject&rdquo; files.  An assumption would need to be made here to link these two set of files.<br/></li>
</ol>

<h4>Generating metadata from the third folder level of datasets provided</h4>

<pre><code class="r">dim(body_acc_x_train)
</code></pre>

<pre><code>## [1] 7352  128
</code></pre>

<pre><code class="r">dim(body_acc_x_test)
</code></pre>

<pre><code>## [1] 2947  128
</code></pre>

<pre><code class="r">dim(body_acc_y_train)
</code></pre>

<pre><code>## [1] 7352  128
</code></pre>

<pre><code class="r">dim(body_acc_y_test)
</code></pre>

<pre><code>## [1] 2947  128
</code></pre>

<pre><code class="r">dim(body_acc_z_train)
</code></pre>

<pre><code>## [1] 7352  128
</code></pre>

<pre><code class="r">dim(body_acc_z_test)
</code></pre>

<pre><code>## [1] 2947  128
</code></pre>

<pre><code class="r">dim(body_gyro_x_train)
</code></pre>

<pre><code>## [1] 7352  128
</code></pre>

<pre><code class="r">dim(body_gyro_x_test)
</code></pre>

<pre><code>## [1] 2947  128
</code></pre>

<pre><code class="r">dim(body_gyro_y_train)
</code></pre>

<pre><code>## [1] 7352  128
</code></pre>

<pre><code class="r">dim(body_gyro_y_test)
</code></pre>

<pre><code>## [1] 2947  128
</code></pre>

<pre><code class="r">dim(body_gyro_z_train)
</code></pre>

<pre><code>## [1] 7352  128
</code></pre>

<pre><code class="r">dim(body_gyro_z_test)
</code></pre>

<pre><code>## [1] 2947  128
</code></pre>

<pre><code class="r">dim(total_acc_x_train)
</code></pre>

<pre><code>## [1] 7352  128
</code></pre>

<pre><code class="r">dim(total_acc_x_test)
</code></pre>

<pre><code>## [1] 2947  128
</code></pre>

<pre><code class="r">dim(total_acc_y_train)
</code></pre>

<pre><code>## [1] 7352  128
</code></pre>

<pre><code class="r">dim(total_acc_y_test)
</code></pre>

<pre><code>## [1] 2947  128
</code></pre>

<pre><code class="r">dim(total_acc_z_train)
</code></pre>

<pre><code>## [1] 7352  128
</code></pre>

<pre><code class="r">dim(total_acc_z_test)
</code></pre>

<pre><code>## [1] 2947  128
</code></pre>

<h3>Analysis of Metadata of the third folder level of datasets</h3>

<ol>
<li> There are 3 sets of files here, each represent either an X, Y, or Z axis of the gyro, accelerator (body estimate) and acceleration(triaxial)</li>
<li> Each of the 9 files in &ldquo;train&rdquo; folder and 9 files in &ldquo;test&rdquo; folder consist of 128 columns - potentially representing each of the sample taken in a reading window.</li>
<li> There are 7352 observations in each of 9 files in &ldquo;train&rdquo; folder, and 2947 observations in each of the 9 files in the &ldquo;test&rdquo; folder.  Potentially, each record matches one observation in the &ldquo;X&rdquo; files found in the second folder level of datasets. However, there also no identifiers available to match the records.</li>
</ol>

<h2>Assumptions for combining the datasets into a file (based on the above observations)</h2>

<ol>
<li> The 561 columns in the &ldquo;X&rdquo; set of files are measurements of the features found in features data, and each column number corresponds to each row number in the features data.  That is, column 1 in a &ldquo;X&rdquo; file is the measure for row 1 in the &ldquo;features&rdquo; file, column 2 in &ldquo;X&rdquo; file is the measure for row 2 in the &ldquo;features&rdquo; file, and so forth until column 561 in a &ldquo;X&rdquo; file is the measure for row 561 in the &ldquo;features&rdquo; file.</li>
<li> The values in &ldquo;y&rdquo; files corresponds to the &ldquo;index&rdquo; value found in the activity_label file. For example, if the value in a &ldquo;y&rdquo; file (column 1) is 5, it refers to the activity &ldquo;STANDING&rdquo;, or a value of 2 would refer to the activity &ldquo;WALKING_UPSTAIRS&rdquo;, and so forth.</li>
<li> As there are no matching key to link the X and Y files, and given that the number of observations in the &ldquo;y&rdquo; files corresponds to the number of observations in the &ldquo;X&rdquo; files, it is assumed that each row (number) of observation in &ldquo;X&rdquo; files corresponds to the same row (number) of observation in &ldquo;y&rdquo; file.  So row 1 of X_train would correspond (and linked) to row 1 of y_train.</li>
<li> Given that there are 30 unique values in the &ldquo;subject&rdquo; files, each unique value refers to the Subject identifier.</li>
<li> As there are no matching key to link the X and Y files, and given that the number of observations in the &ldquo;subject files corresponds to the number of observations in &quot;X&rdquo; files, it is assumed that each row in &ldquo;subject&rdquo; files corresponds (and linked) to the same row in &ldquo;X&rdquo; files.  That is row 1 in subject_train would correspond to row 1 in X_train, and so forth.</li>
<li> As there are 128 columns in the files from the third level folders (Inertial), and there are 128 reading/windows, it is assumed that each column refers to a reading/window for that sensor axis of x, y, and z.</li>
<li> As there are the same number of observations for each of the files to the number of observations in the respective &ldquo;X&rdquo; files (X_train and X_test), it is assumed that each row in these Inertial files corresponds (and linked to ) each row in their respective &ldquo;X&rdquo; files.</li>
</ol>

<h2>Resulting Design for the single data file</h2>

<p>The resulting combined file would consist of the following columns:<br/>
1.  Subject ID (1) - from &ldquo;subject&rdquo; files,<br/>
2.  Activity (1) - from &ldquo;y&rdquo; files,<br/>
3.  Features (561) - from &ldquo;X&rdquo; files,<br/>
4.  Body<em>Giro (128 x 3) - from &ldquo;body_giro</em>&rdquo; x, y, and z files,<br/>
5.  Body<em>Acc (128 x 3) - from &ldquo;body_acc</em>&rdquo; x, y, and z files,<br/>
6.  Total<em>Acc (128 x 3) - from &ldquo;total_acc</em>&rdquo; x, y and z files  </p>

<p>In total, there will be 1715 columns.</p>

<p>The total number of observations would be 7352 + 2947 = 10299 observations  </p>

<h2>Resulting Design for the Tidy data file</h2>

<p>As the project calls for submission of the mean and standard deviation variables, this would automatically exclude data from the Inertial data, as these are readings from the 128 windows.  The resulting merge and summarised file would consist of only the following columns:<br/>
1.  Subject ID (1) - from &ldquo;subject&rdquo; files,<br/>
2.  Activity (1) - from &ldquo;y&rdquo; files factored with labels from activity_labels,<br/>
3.  Means and Standard Deviation of Features (86 features) - from both &ldquo;X&rdquo; files  </p>

<p>Please refer to codebook.md for the Data Dictionary of the Tidy data file.  </p>

<p>The following R code provides the code to combine and generate the file based on the above designs:</p>

<pre><code class="r">## Common functions to be used

# Set names
set_colnames &lt;- function (x, newnames) {
    oldnames &lt;- names(x)
    setnames(x, oldnames, newnames)
}

# Merge data frames so that the row orders are not changed after the merge.
# This is done by creating an new column row_id and merging based on that row_id.
# Then the row_id is removed.  From analysis of the data files, there are no column
# with the name row_id.
merge_by_row &lt;- function(x, y, ...) {
    add_id &lt;- function(data) {
        data.frame(data, rec_id = seq_len(nrow(data)))
    }
    remove_id &lt;- function(data) {
        data_col &lt;- colnames(data) != &quot;rec_id&quot;
        data[, data_col]
    }
    tmp_x &lt;- add_id(x)
    tmp_y &lt;- add_id(y)
    tmp_xy &lt;- merge(tmp_x, tmp_y)
    remove_id(tmp_xy)
}

## Start by setting the column names first for the key datasets
set_colnames(subject_train, &quot;Subject_ID&quot;)
set_colnames(subject_test, &quot;Subject_ID&quot;)

set_colnames(y_train, &quot;Activity&quot;)
set_colnames(y_test, &quot;Activity&quot;)

# Set the activity_labels column names to match for y_train and y_test header
# to facilitate merging later.
set_colnames(activity_labels, c(&quot;Activity_ID&quot;, &quot;Activity&quot;))

## Strip away special characters in features&#39; names
feature_names &lt;- gsub(&quot;\\.&quot;, &quot;_&quot;, make.names(features$V2))

set_colnames(x_train, as.vector(feature_names))
set_colnames(x_test, as.vector(feature_names))

# Arbitrarily set the column names for each senor data for the 128 reading windows.

inertial_labels &lt;-  as.vector(paste(&quot;body_gyro_x_&quot;, as.character(seq_len(128)), sep=&quot;&quot;))
set_colnames(body_gyro_x_train, inertial_labels)
set_colnames(body_gyro_x_test, inertial_labels)

inertial_labels &lt;-  as.vector(paste(&quot;body_gyro_y_&quot;, as.character(seq_len(128)), sep=&quot;&quot;))
set_colnames(body_gyro_y_train, inertial_labels)
set_colnames(body_gyro_y_test, inertial_labels)

inertial_labels &lt;-  as.vector(paste(&quot;body_gyro_z_&quot;, as.character(seq_len(128)), sep=&quot;&quot;))
set_colnames(body_gyro_z_train, inertial_labels)
set_colnames(body_gyro_z_test, inertial_labels)

inertial_labels &lt;-  as.vector(paste(&quot;body_acc_x_&quot;, as.character(seq_len(128)), sep=&quot;&quot;))
set_colnames(body_acc_x_train, inertial_labels)
set_colnames(body_acc_x_test, inertial_labels)

inertial_labels &lt;-  as.vector(paste(&quot;body_acc_y_&quot;, as.character(seq_len(128)), sep=&quot;&quot;))
set_colnames(body_acc_y_train, inertial_labels)
set_colnames(body_acc_y_test, inertial_labels)

inertial_labels &lt;-  as.vector(paste(&quot;body_acc_z_&quot;, as.character(seq_len(128)), sep=&quot;&quot;))
set_colnames(body_acc_z_train, inertial_labels)
set_colnames(body_acc_z_test, inertial_labels)

inertial_labels &lt;-  as.vector(paste(&quot;total_acc_x_&quot;, as.character(seq_len(128)), sep=&quot;&quot;))
set_colnames(total_acc_x_train, inertial_labels)
set_colnames(total_acc_x_test, inertial_labels)

inertial_labels &lt;-  as.vector(paste(&quot;total_acc_y_&quot;, as.character(seq_len(128)), sep=&quot;&quot;))
set_colnames(total_acc_y_train, inertial_labels)
set_colnames(total_acc_y_test, inertial_labels)

inertial_labels &lt;-  as.vector(paste(&quot;total_acc_z_&quot;, as.character(seq_len(128)), sep=&quot;&quot;))
set_colnames(total_acc_z_train, inertial_labels)
set_colnames(total_acc_z_test, inertial_labels)

## Merge all the train data
# merge main train data
merge_df_main_train &lt;- merge_by_row (subject_train, y_train)
merge_df_main_train &lt;- merge_by_row (merge_df_main_train, x_train)

# merge inertial train data
merge_df_train_all &lt;- merge_by_row (merge_df_main_train, body_gyro_x_train)
merge_df_train_all &lt;- merge_by_row (merge_df_train_all, body_gyro_y_train)
merge_df_train_all &lt;- merge_by_row (merge_df_train_all, body_gyro_z_train)
merge_df_train_all &lt;- merge_by_row (merge_df_train_all, body_acc_x_train)
merge_df_train_all &lt;- merge_by_row (merge_df_train_all, body_acc_y_train)
merge_df_train_all &lt;- merge_by_row (merge_df_train_all, body_acc_z_train)
merge_df_train_all &lt;- merge_by_row (merge_df_train_all, total_acc_x_train)
merge_df_train_all &lt;- merge_by_row (merge_df_train_all, total_acc_y_train)
merge_df_train_all &lt;- merge_by_row (merge_df_train_all, total_acc_z_train)

## Merge all the test data
# merge main test data
merge_df_main_test &lt;- merge_by_row (subject_test, y_test)
merge_df_main_test &lt;- merge_by_row (merge_df_main_test, x_test)

# merge inertial test test
merge_df_test_all &lt;- merge_by_row (merge_df_main_test, body_gyro_x_test)
merge_df_test_all &lt;- merge_by_row (merge_df_test_all, body_gyro_y_test)
merge_df_test_all &lt;- merge_by_row (merge_df_test_all, body_gyro_z_test)
merge_df_test_all &lt;- merge_by_row (merge_df_test_all, body_acc_x_test)
merge_df_test_all &lt;- merge_by_row (merge_df_test_all, body_acc_y_test)
merge_df_test_all &lt;- merge_by_row (merge_df_test_all, body_acc_z_test)
merge_df_test_all &lt;- merge_by_row (merge_df_test_all, total_acc_x_test)
merge_df_test_all &lt;- merge_by_row (merge_df_test_all, total_acc_y_test)
merge_df_test_all &lt;- merge_by_row (merge_df_test_all, total_acc_z_test)

## Merge both Train and Test files into a single dataset

merge_list &lt;- list(merge_df_train_all, merge_df_test_all)
merge_df_all &lt;- rbindlist(merge_list)

merge_list &lt;- list(merge_df_main_train, merge_df_main_test)
merge_df_main_all &lt;- rbindlist(merge_list)
</code></pre>

<p>The merged train and test datasets, including Inertial data, are in the dataset: merge_df_all.  </p>

<p>The merged train and test datasets, excluding Inertial data, are in the dataset: merge_df_main_all.  This will be used to generate the final Tidy data file.  </p>

<p>Extracting only the measurements on the mean and standard deviation for each measurement for all the features, such as acceleration, angle, jerk, etc..</p>

<pre><code class="r">## Retrieve columns that are means or standard deviations (std)
# Using only merge_df_main_all as the Inertial data would not have mean or 
# standard deviations given the column names are arbitrarily given.
mean_std_col &lt;- names(merge_df_main_all)[grepl(&quot;mean|std&quot;, names(merge_df_main_all), ignore.case = TRUE)]

mean_std_col &lt;- c(&quot;Subject_ID&quot;, &quot;Activity&quot;, mean_std_col)

merge_df_select &lt;- select(merge_df_main_all, one_of(mean_std_col))
</code></pre>

<p>The extracted measurements on the mean and standard deviation is now in the data set: merge_df_select.  </p>

<p>Changing the numeric activity values to appropriate labels found in activity_labels.  Assuming that 
each number corresponds to number in that file. </p>

<pre><code class="r">## Appropriately labels the data set with descriptive variable names. 
merge_df_select$Activity &lt;- factor(merge_df_select$Activity,
                                   levels=activity_labels$Activity_ID,
                                   labels=activity_labels$Activity)
</code></pre>

<p>Creating a new tidy data set with the average of each variable for each activity and each subject</p>

<pre><code class="r">## Creates an independent tidy data set with the average of each variable for each activity and each subject.

merge_select_means &lt;- merge_df_select %&gt;% group_by(Subject_ID, Activity) %&gt;%
    summarise_each(funs(mean))
</code></pre>

<h2>Final Data File Generated</h2>

<p>The new dataset is: merge_tidy.txt.<br/>
This file contains all the averages of the means and standard deviations of all the features of accelertion, gyro, jerk, etc, group by the Subject and the Activity.</p>

<pre><code class="r">## print(merge_select_means)
# write.csv(merge_select_means, file=&quot;merge_select_means.csv&quot;, quote=FALSE)

## Instead the following file would be created for submission: merge_tidy.txt
write.table(merge_select_means, file=&quot;merge_tidy.txt&quot;, row.name=FALSE)
</code></pre>

<h2>Code Book</h2>

<p>Please refer to the codebook.md for the generated file&#39;s fields.</p>

</body>

</html>
